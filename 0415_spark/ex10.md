
```py
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
def updateCount(newCounts, state):
    if state == None: return sum(newCounts)
    else: return state + sum(newCounts)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print >> sys.stderr, "Usage: StreamingLogsMB.py <hostname> <port>"
        sys.exit(-1)
    
    # get hostname and port of data source from application arguments
    hostname = sys.argv[1]
    port = int(sys.argv[2])
     
    # Create a new SparkContext
    sc = SparkContext()

    # Set log level to ERROR to avoid distracting extra output
    sc.setLogLevel("ERROR")

    # Create and configure a new Streaming Context 
    # with a 1 second batch duration
    ssc = StreamingContext(sc,1)

    # Create a DStream of log data from the server and port specified    
    logs = ssc.socketTextStream(hostname,port)

    # TODO
    print "Solution not yet implemented"
    
    
    # data mixing
    reqByUser = logs.map(lambda stream : (stream.split(' ')[2], 1) ).reduceByKeyAndWindow (lambda v1, v2: v1+v2, 5, 2)
    rankByReq = reqByUser.map(lambda (k,v):(v,k)).transform(lambda rdd: rdd.sortByKey(False)).map(lambda (k,v):(v,k))
    #rankByReq.pprint(5)
    
    # checkpoint directory enable.
    ssc.checkpoint("logcheckpt")
    
    rankUpdate = rankByReq.updateStateByKey( lambda newCounts, state: updateCount(newCounts, state))
    rankByReqN = rankUpdate.map(lambda (k,v):(v,k)).transform(lambda rdd: rdd.sortByKey(False)).map(lambda (k,v):(v,k))
    rankByReqN.pprint(5)
    
    ssc.start()
    ssc.awaitTermination()
```    